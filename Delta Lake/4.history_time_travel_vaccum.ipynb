{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060a24b8-bc87-4b52-84ca-7db1f1e74ce7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>3</td><td>2024-02-23T14:45:42.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>MERGE</td><td>Map(predicate -> [\"(cast(driverId#3080 as bigint) = driverId#88L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}])</td><td>null</td><td>List(2942660207359433)</td><td>0118-233830-ke242ohy</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, executionTimeMs -> 1884, numTargetRowsInserted -> 5, scanTimeMs -> 940, numTargetRowsUpdated -> 5, numOutputRows -> 10, numTargetChangeFilesAdded -> 0, numSourceRows -> 10, numTargetFilesRemoved -> 1, rewriteTimeMs -> 670)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>2</td><td>2024-02-23T14:16:12.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>MERGE</td><td>Map(predicate -> [\"(cast(driverId#1884 as bigint) = driverId#55L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}])</td><td>null</td><td>List(2942660207359433)</td><td>0118-233830-ke242ohy</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 5, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, executionTimeMs -> 2896, numTargetRowsInserted -> 5, scanTimeMs -> 1304, numTargetRowsUpdated -> 5, numOutputRows -> 15, numTargetChangeFilesAdded -> 0, numSourceRows -> 10, numTargetFilesRemoved -> 1, rewriteTimeMs -> 920)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>1</td><td>2024-02-23T14:14:45.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>MERGE</td><td>Map(predicate -> [\"(cast(driverId#831 as bigint) = driverId#24L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}])</td><td>null</td><td>List(2942660207359433)</td><td>0118-233830-ke242ohy</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, executionTimeMs -> 4618, numTargetRowsInserted -> 10, scanTimeMs -> 1835, numTargetRowsUpdated -> 0, numOutputRows -> 10, numTargetChangeFilesAdded -> 0, numSourceRows -> 10, numTargetFilesRemoved -> 0, rewriteTimeMs -> 2193)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>0</td><td>2024-02-23T14:05:50.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>CREATE TABLE</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(2942660207359433)</td><td>0118-233830-ke242ohy</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "2024-02-23T14:45:42.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "MERGE",
         {
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedPredicates": "[{\"actionType\":\"insert\"}]",
          "predicate": "[\"(cast(driverId#3080 as bigint) = driverId#88L)\"]"
         },
         null,
         [
          "2942660207359433"
         ],
         "0118-233830-ke242ohy",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "1884",
          "numOutputRows": "10",
          "numSourceRows": "10",
          "numTargetChangeFilesAdded": "0",
          "numTargetFilesAdded": "1",
          "numTargetFilesRemoved": "1",
          "numTargetRowsCopied": "0",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "5",
          "numTargetRowsUpdated": "5",
          "rewriteTimeMs": "670",
          "scanTimeMs": "940"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         2,
         "2024-02-23T14:16:12.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "MERGE",
         {
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedPredicates": "[{\"actionType\":\"insert\"}]",
          "predicate": "[\"(cast(driverId#1884 as bigint) = driverId#55L)\"]"
         },
         null,
         [
          "2942660207359433"
         ],
         "0118-233830-ke242ohy",
         1,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "2896",
          "numOutputRows": "15",
          "numSourceRows": "10",
          "numTargetChangeFilesAdded": "0",
          "numTargetFilesAdded": "2",
          "numTargetFilesRemoved": "1",
          "numTargetRowsCopied": "5",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "5",
          "numTargetRowsUpdated": "5",
          "rewriteTimeMs": "920",
          "scanTimeMs": "1304"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         1,
         "2024-02-23T14:14:45.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "MERGE",
         {
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedPredicates": "[{\"actionType\":\"insert\"}]",
          "predicate": "[\"(cast(driverId#831 as bigint) = driverId#24L)\"]"
         },
         null,
         [
          "2942660207359433"
         ],
         "0118-233830-ke242ohy",
         0,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "4618",
          "numOutputRows": "10",
          "numSourceRows": "10",
          "numTargetChangeFilesAdded": "0",
          "numTargetFilesAdded": "1",
          "numTargetFilesRemoved": "0",
          "numTargetRowsCopied": "0",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "10",
          "numTargetRowsUpdated": "0",
          "rewriteTimeMs": "2193",
          "scanTimeMs": "1835"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         0,
         "2024-02-23T14:05:50.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "CREATE TABLE",
         {
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{}"
         },
         null,
         [
          "2942660207359433"
         ],
         "0118-233830-ke242ohy",
         null,
         "WriteSerializable",
         true,
         {},
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "describe history f1_delta.drivers_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d398b6-026e-43fe-8a42-271d39f79da5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>driverId</th><th>dob</th><th>forename</th><th>surname</th><th>createDate</th><th>updateDate</th></tr></thead><tbody><tr><td>6</td><td>1985-01-11</td><td>KAZUKI</td><td>NAKAJIMA</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>7</td><td>1979-02-28</td><td>SÉBASTIEN</td><td>BOURDAIS</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>8</td><td>1979-10-17</td><td>KIMI</td><td>RÄIKKÖNEN</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>9</td><td>1984-12-07</td><td>ROBERT</td><td>KUBICA</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>10</td><td>1982-03-18</td><td>TIMO</td><td>GLOCK</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>11</td><td>1977-01-28</td><td>TAKUMA</td><td>SATO</td><td>2024-02-23</td><td>null</td></tr><tr><td>12</td><td>1985-07-25</td><td>NELSON</td><td>PIQUET JR.</td><td>2024-02-23</td><td>null</td></tr><tr><td>13</td><td>1981-04-25</td><td>FELIPE</td><td>MASSA</td><td>2024-02-23</td><td>null</td></tr><tr><td>14</td><td>1971-03-27</td><td>DAVID</td><td>COULTHARD</td><td>2024-02-23</td><td>null</td></tr><tr><td>15</td><td>1974-07-13</td><td>JARNO</td><td>TRULLI</td><td>2024-02-23</td><td>null</td></tr><tr><td>1</td><td>1985-01-07</td><td>LEWIS</td><td>HAMILTON</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>2</td><td>1977-05-10</td><td>NICK</td><td>HEIDFELD</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>3</td><td>1985-06-27</td><td>NICO</td><td>ROSBERG</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>4</td><td>1981-07-29</td><td>FERNANDO</td><td>ALONSO</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>5</td><td>1981-10-19</td><td>HEIKKI</td><td>KOVALAINEN</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>16</td><td>1983-01-11</td><td>ADRIAN</td><td>SUTIL</td><td>2024-02-23</td><td>null</td></tr><tr><td>17</td><td>1976-08-27</td><td>MARK</td><td>WEBBER</td><td>2024-02-23</td><td>null</td></tr><tr><td>18</td><td>1980-01-19</td><td>JENSON</td><td>BUTTON</td><td>2024-02-23</td><td>null</td></tr><tr><td>19</td><td>1979-04-18</td><td>ANTHONY</td><td>DAVIDSON</td><td>2024-02-23</td><td>null</td></tr><tr><td>20</td><td>1987-07-03</td><td>SEBASTIAN</td><td>VETTEL</td><td>2024-02-23</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6,
         "1985-01-11",
         "KAZUKI",
         "NAKAJIMA",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         7,
         "1979-02-28",
         "SÉBASTIEN",
         "BOURDAIS",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         8,
         "1979-10-17",
         "KIMI",
         "RÄIKKÖNEN",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         9,
         "1984-12-07",
         "ROBERT",
         "KUBICA",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         10,
         "1982-03-18",
         "TIMO",
         "GLOCK",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         11,
         "1977-01-28",
         "TAKUMA",
         "SATO",
         "2024-02-23",
         null
        ],
        [
         12,
         "1985-07-25",
         "NELSON",
         "PIQUET JR.",
         "2024-02-23",
         null
        ],
        [
         13,
         "1981-04-25",
         "FELIPE",
         "MASSA",
         "2024-02-23",
         null
        ],
        [
         14,
         "1971-03-27",
         "DAVID",
         "COULTHARD",
         "2024-02-23",
         null
        ],
        [
         15,
         "1974-07-13",
         "JARNO",
         "TRULLI",
         "2024-02-23",
         null
        ],
        [
         1,
         "1985-01-07",
         "LEWIS",
         "HAMILTON",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         2,
         "1977-05-10",
         "NICK",
         "HEIDFELD",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         3,
         "1985-06-27",
         "NICO",
         "ROSBERG",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         4,
         "1981-07-29",
         "FERNANDO",
         "ALONSO",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         5,
         "1981-10-19",
         "HEIKKI",
         "KOVALAINEN",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         16,
         "1983-01-11",
         "ADRIAN",
         "SUTIL",
         "2024-02-23",
         null
        ],
        [
         17,
         "1976-08-27",
         "MARK",
         "WEBBER",
         "2024-02-23",
         null
        ],
        [
         18,
         "1980-01-19",
         "JENSON",
         "BUTTON",
         "2024-02-23",
         null
        ],
        [
         19,
         "1979-04-18",
         "ANTHONY",
         "DAVIDSON",
         "2024-02-23",
         null
        ],
        [
         20,
         "1987-07-03",
         "SEBASTIAN",
         "VETTEL",
         "2024-02-23",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "driverId",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "forename",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "surname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "createDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "updateDate",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from f1_delta.drivers_merge\n",
    "version as of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20077036-4169-4f5c-9ec6-1f0ae2606cf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>driverId</th><th>dob</th><th>forename</th><th>surname</th><th>createDate</th><th>updateDate</th></tr></thead><tbody><tr><td>6</td><td>1985-01-11</td><td>KAZUKI</td><td>NAKAJIMA</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>7</td><td>1979-02-28</td><td>SÉBASTIEN</td><td>BOURDAIS</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>8</td><td>1979-10-17</td><td>KIMI</td><td>RÄIKKÖNEN</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>9</td><td>1984-12-07</td><td>ROBERT</td><td>KUBICA</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>10</td><td>1982-03-18</td><td>TIMO</td><td>GLOCK</td><td>2024-02-23</td><td>2024-02-23</td></tr><tr><td>11</td><td>1977-01-28</td><td>TAKUMA</td><td>SATO</td><td>2024-02-23</td><td>null</td></tr><tr><td>12</td><td>1985-07-25</td><td>NELSON</td><td>PIQUET JR.</td><td>2024-02-23</td><td>null</td></tr><tr><td>13</td><td>1981-04-25</td><td>FELIPE</td><td>MASSA</td><td>2024-02-23</td><td>null</td></tr><tr><td>14</td><td>1971-03-27</td><td>DAVID</td><td>COULTHARD</td><td>2024-02-23</td><td>null</td></tr><tr><td>15</td><td>1974-07-13</td><td>JARNO</td><td>TRULLI</td><td>2024-02-23</td><td>null</td></tr><tr><td>1</td><td>1985-01-07</td><td>Lewis</td><td>Hamilton</td><td>2024-02-23</td><td>null</td></tr><tr><td>2</td><td>1977-05-10</td><td>Nick</td><td>Heidfeld</td><td>2024-02-23</td><td>null</td></tr><tr><td>3</td><td>1985-06-27</td><td>Nico</td><td>Rosberg</td><td>2024-02-23</td><td>null</td></tr><tr><td>4</td><td>1981-07-29</td><td>Fernando</td><td>Alonso</td><td>2024-02-23</td><td>null</td></tr><tr><td>5</td><td>1981-10-19</td><td>Heikki</td><td>Kovalainen</td><td>2024-02-23</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         6,
         "1985-01-11",
         "KAZUKI",
         "NAKAJIMA",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         7,
         "1979-02-28",
         "SÉBASTIEN",
         "BOURDAIS",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         8,
         "1979-10-17",
         "KIMI",
         "RÄIKKÖNEN",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         9,
         "1984-12-07",
         "ROBERT",
         "KUBICA",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         10,
         "1982-03-18",
         "TIMO",
         "GLOCK",
         "2024-02-23",
         "2024-02-23"
        ],
        [
         11,
         "1977-01-28",
         "TAKUMA",
         "SATO",
         "2024-02-23",
         null
        ],
        [
         12,
         "1985-07-25",
         "NELSON",
         "PIQUET JR.",
         "2024-02-23",
         null
        ],
        [
         13,
         "1981-04-25",
         "FELIPE",
         "MASSA",
         "2024-02-23",
         null
        ],
        [
         14,
         "1971-03-27",
         "DAVID",
         "COULTHARD",
         "2024-02-23",
         null
        ],
        [
         15,
         "1974-07-13",
         "JARNO",
         "TRULLI",
         "2024-02-23",
         null
        ],
        [
         1,
         "1985-01-07",
         "Lewis",
         "Hamilton",
         "2024-02-23",
         null
        ],
        [
         2,
         "1977-05-10",
         "Nick",
         "Heidfeld",
         "2024-02-23",
         null
        ],
        [
         3,
         "1985-06-27",
         "Nico",
         "Rosberg",
         "2024-02-23",
         null
        ],
        [
         4,
         "1981-07-29",
         "Fernando",
         "Alonso",
         "2024-02-23",
         null
        ],
        [
         5,
         "1981-10-19",
         "Heikki",
         "Kovalainen",
         "2024-02-23",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "driverId",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dob",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "forename",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "surname",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "createDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "updateDate",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from f1_delta.drivers_merge\n",
    "timestamp as of \"2024-02-23T14:16:12.000+0000\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76f89466-632c-4a24-9927-d439b0a6b900",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('delta').option('timestampAsOf', \"2024-02-23T14:14:45.000+0000\").load(\"abfss://demo@dlformulaone2024.dfs.core.windows.net/drivers_merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2577810-a0e8-40aa-bf80-b6af17abc70e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "vaccum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7463e68-2ee0-42ae-aa57-288f115093b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>abfss://demo@dlformulaone2024.dfs.core.windows.net/drivers_merge</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "abfss://demo@dlformulaone2024.dfs.core.windows.net/drivers_merge"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "vacuum f1_delta.drivers_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19f7321-2c28-4dcf-a08c-6eb6f36a911a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>7</td><td>2024-02-26T01:38:19.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>VACUUM END</td><td>Map(status -> COMPLETED)</td><td>null</td><td>List(2942660207359453)</td><td>0118-233830-ke242ohy</td><td>6</td><td>SnapshotIsolation</td><td>true</td><td>Map(numDeletedFiles -> 3, numVacuumedDirectories -> 1)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>6</td><td>2024-02-26T01:38:16.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>VACUUM START</td><td>Map(retentionCheckEnabled -> false, specifiedRetentionMillis -> 0, defaultRetentionMillis -> 604800000)</td><td>null</td><td>List(2942660207359453)</td><td>0118-233830-ke242ohy</td><td>5</td><td>SnapshotIsolation</td><td>true</td><td>Map(numFilesToDelete -> 3, sizeOfDataToDelete -> 20133)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>5</td><td>2024-02-26T01:36:35.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>VACUUM END</td><td>Map(status -> COMPLETED)</td><td>null</td><td>List(2942660207359453)</td><td>0118-233830-ke242ohy</td><td>4</td><td>SnapshotIsolation</td><td>true</td><td>Map(numDeletedFiles -> 0, numVacuumedDirectories -> 1)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>4</td><td>2024-02-26T01:36:26.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>VACUUM START</td><td>Map(retentionCheckEnabled -> true, defaultRetentionMillis -> 604800000)</td><td>null</td><td>List(2942660207359453)</td><td>0118-233830-ke242ohy</td><td>3</td><td>SnapshotIsolation</td><td>true</td><td>Map(numFilesToDelete -> 0, sizeOfDataToDelete -> 0)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>3</td><td>2024-02-23T10:58:11.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>DELETE</td><td>Map(predicate -> [\"(cast(position#6907 as int) <= 10)\"])</td><td>null</td><td>List(2942660207359425)</td><td>0118-233830-ke242ohy</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 10, numAddedChangeFiles -> 0, executionTimeMs -> 1309, numDeletedRows -> 10, scanTimeMs -> 641, numAddedFiles -> 1, rewriteTimeMs -> 668)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>2</td><td>2024-02-23T10:41:13.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>UPDATE</td><td>Map(predicate -> [\"(cast(position#5608 as int) <= 10)\"])</td><td>null</td><td>List(2942660207359425)</td><td>0118-233830-ke242ohy</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 10, numAddedChangeFiles -> 0, executionTimeMs -> 1003, scanTimeMs -> 441, numAddedFiles -> 1, numUpdatedRows -> 10, rewriteTimeMs -> 562)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>1</td><td>2024-02-23T10:31:44.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>UPDATE</td><td>Map(predicate -> [\"(cast(position#4332 as int) <= 10)\"])</td><td>null</td><td>List(2942660207359425)</td><td>0118-233830-ke242ohy</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 10, numAddedChangeFiles -> 0, executionTimeMs -> 1591, scanTimeMs -> 908, numAddedFiles -> 1, numUpdatedRows -> 10, rewriteTimeMs -> 672)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr><tr><td>0</td><td>2024-02-23T10:07:57.000+0000</td><td>3998316786820796</td><td>shakib.polock@outlook.com</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(2942660207359415)</td><td>0118-233830-ke242ohy</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputRows -> 20, numOutputBytes -> 6711)</td><td>null</td><td>Databricks-Runtime/11.3.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7,
         "2024-02-26T01:38:19.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "VACUUM END",
         {
          "status": "COMPLETED"
         },
         null,
         [
          "2942660207359453"
         ],
         "0118-233830-ke242ohy",
         6,
         "SnapshotIsolation",
         true,
         {
          "numDeletedFiles": "3",
          "numVacuumedDirectories": "1"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         6,
         "2024-02-26T01:38:16.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "VACUUM START",
         {
          "defaultRetentionMillis": "604800000",
          "retentionCheckEnabled": "false",
          "specifiedRetentionMillis": "0"
         },
         null,
         [
          "2942660207359453"
         ],
         "0118-233830-ke242ohy",
         5,
         "SnapshotIsolation",
         true,
         {
          "numFilesToDelete": "3",
          "sizeOfDataToDelete": "20133"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         5,
         "2024-02-26T01:36:35.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "VACUUM END",
         {
          "status": "COMPLETED"
         },
         null,
         [
          "2942660207359453"
         ],
         "0118-233830-ke242ohy",
         4,
         "SnapshotIsolation",
         true,
         {
          "numDeletedFiles": "0",
          "numVacuumedDirectories": "1"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         4,
         "2024-02-26T01:36:26.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "VACUUM START",
         {
          "defaultRetentionMillis": "604800000",
          "retentionCheckEnabled": "true"
         },
         null,
         [
          "2942660207359453"
         ],
         "0118-233830-ke242ohy",
         3,
         "SnapshotIsolation",
         true,
         {
          "numFilesToDelete": "0",
          "sizeOfDataToDelete": "0"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         3,
         "2024-02-23T10:58:11.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "DELETE",
         {
          "predicate": "[\"(cast(position#6907 as int) <= 10)\"]"
         },
         null,
         [
          "2942660207359425"
         ],
         "0118-233830-ke242ohy",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "1309",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "10",
          "numDeletedRows": "10",
          "numRemovedFiles": "1",
          "rewriteTimeMs": "668",
          "scanTimeMs": "641"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         2,
         "2024-02-23T10:41:13.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "UPDATE",
         {
          "predicate": "[\"(cast(position#5608 as int) <= 10)\"]"
         },
         null,
         [
          "2942660207359425"
         ],
         "0118-233830-ke242ohy",
         1,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "1003",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "10",
          "numRemovedFiles": "1",
          "numUpdatedRows": "10",
          "rewriteTimeMs": "562",
          "scanTimeMs": "441"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         1,
         "2024-02-23T10:31:44.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "UPDATE",
         {
          "predicate": "[\"(cast(position#4332 as int) <= 10)\"]"
         },
         null,
         [
          "2942660207359425"
         ],
         "0118-233830-ke242ohy",
         0,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "1591",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "10",
          "numRemovedFiles": "1",
          "numUpdatedRows": "10",
          "rewriteTimeMs": "672",
          "scanTimeMs": "908"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ],
        [
         0,
         "2024-02-23T10:07:57.000+0000",
         "3998316786820796",
         "shakib.polock@outlook.com",
         "CREATE OR REPLACE TABLE AS SELECT",
         {
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{}"
         },
         null,
         [
          "2942660207359415"
         ],
         "0118-233830-ke242ohy",
         null,
         "WriteSerializable",
         false,
         {
          "numFiles": "1",
          "numOutputBytes": "6711",
          "numOutputRows": "20"
         },
         null,
         "Databricks-Runtime/11.3.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--select * from f1_delta.result_managed\n",
    "describe history f1_delta.result_managed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a330400a-cf56-47b5-aae2-fda9d857aa79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>abfss://demo@dlformulaone2024.dfs.core.windows.net/result_managed</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "abfss://demo@dlformulaone2024.dfs.core.windows.net/result_managed"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "set spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "vacuum f1_delta.result_managed retain 0 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbcbd5cd-7374-47a0-87e4-76427bcd517a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 276.0 failed 4 times, most recent failure: Lost task 0.3 in stage 276.0 (TID 3222) (10.139.64.4 executor driver): com.databricks.sql.io.FileReadException: Error while reading file abfss:REDACTED_LOCAL_PART@dlformulaone2024.dfs.core.windows.net/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet. File abfss://demo@dlformulaone2024.dfs.core.windows.net/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:654)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:602)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:747)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:456)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:451)\n",
       "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1983)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:186)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dlformulaone2024.dfs.core.windows.net/demo/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet?timeout=90, PathNotFound, \"The specified path does not exist. RequestId:1a85ef3b-e01f-0093-2858-68792a000000 Time:2024-02-26T02:05:28.3676575Z\"\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:617)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:566)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:360)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:298)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:265)\n",
       "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.$anonfun$read$2(FileSystemWithMetrics.scala:214)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeMetric(FileSystemWithMetrics.scala:158)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesMetric(FileSystemWithMetrics.scala:178)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.$anonfun$withTimeAndBytesReadMetric$1(FileSystemWithMetrics.scala:192)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat com.databricks.spark.metrics.SamplerWithPeriod.sample(FileSystemWithMetrics.scala:85)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:192)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric$(FileSystemWithMetrics.scala:191)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:199)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.read(FileSystemWithMetrics.scala:213)\n",
       "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
       "\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterBytesFromInputStream(CachingParquetFooterReader.java:135)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:374)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$LegacyCache.read(CachingParquetFooterReader.java:338)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:286)\n",
       "\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:282)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:204)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:396)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:502)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:356)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:573)\n",
       "\t... 37 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3386)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3318)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3309)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3309)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1433)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1433)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1433)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3598)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3536)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3524)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1182)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1170)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2746)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:312)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:271)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:322)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:105)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:112)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:115)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:104)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:88)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:527)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:519)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:539)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:396)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:390)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:292)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:433)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:430)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3431)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3422)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4297)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:773)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4295)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:249)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:399)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:194)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:148)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:349)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4295)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3421)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:723)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:1424)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:505)\n",
       "\tat sun.reflect.GeneratedMethodAccessor527.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.databricks.sql.io.FileReadException: Error while reading file abfss:REDACTED_LOCAL_PART@dlformulaone2024.dfs.core.windows.net/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet. File abfss://demo@dlformulaone2024.dfs.core.windows.net/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:654)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:602)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:747)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:456)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:451)\n",
       "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1983)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:186)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dlformulaone2024.dfs.core.windows.net/demo/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet?timeout=90, PathNotFound, \"The specified path does not exist. RequestId:1a85ef3b-e01f-0093-2858-68792a000000 Time:2024-02-26T02:05:28.3676575Z\"\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:617)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:566)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:360)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:298)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:265)\n",
       "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.$anonfun$read$2(FileSystemWithMetrics.scala:214)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeMetric(FileSystemWithMetrics.scala:158)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesMetric(FileSystemWithMetrics.scala:178)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.$anonfun$withTimeAndBytesReadMetric$1(FileSystemWithMetrics.scala:192)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat com.databricks.spark.metrics.SamplerWithPeriod.sample(FileSystemWithMetrics.scala:85)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:192)\n",
       "\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric$(FileSystemWithMetrics.scala:191)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:199)\n",
       "\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.read(FileSystemWithMetrics.scala:213)\n",
       "\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n",
       "\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterBytesFromInputStream(CachingParquetFooterReader.java:135)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:374)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$LegacyCache.read(CachingParquetFooterReader.java:338)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:286)\n",
       "\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:282)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:204)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:396)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:502)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:356)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:573)\n",
       "\t... 37 more"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 276.0 failed 4 times, most recent failure: Lost task 0.3 in stage 276.0 (TID 3222) (10.139.64.4 executor driver): com.databricks.sql.io.FileReadException: Error while reading file abfss:REDACTED_LOCAL_PART@dlformulaone2024.dfs.core.windows.net/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet. File abfss://demo@dlformulaone2024.dfs.core.windows.net/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:654)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:602)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:747)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:456)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:451)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1983)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:186)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dlformulaone2024.dfs.core.windows.net/demo/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet?timeout=90, PathNotFound, \"The specified path does not exist. RequestId:1a85ef3b-e01f-0093-2858-68792a000000 Time:2024-02-26T02:05:28.3676575Z\"\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:617)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:566)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:360)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:298)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.$anonfun$read$2(FileSystemWithMetrics.scala:214)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeMetric(FileSystemWithMetrics.scala:158)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesMetric(FileSystemWithMetrics.scala:178)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.$anonfun$withTimeAndBytesReadMetric$1(FileSystemWithMetrics.scala:192)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.spark.metrics.SamplerWithPeriod.sample(FileSystemWithMetrics.scala:85)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:192)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric$(FileSystemWithMetrics.scala:191)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:199)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.read(FileSystemWithMetrics.scala:213)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterBytesFromInputStream(CachingParquetFooterReader.java:135)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:374)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$LegacyCache.read(CachingParquetFooterReader.java:338)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:286)\n\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:282)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:204)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:396)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:502)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:356)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:573)\n\t... 37 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3386)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3318)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3309)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3309)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1433)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1433)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3598)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3536)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3524)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1182)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1170)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2746)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:312)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:271)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:322)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:105)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:112)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:115)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:104)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:88)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:527)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:519)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:539)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:396)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:390)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:292)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:433)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:430)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3431)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3422)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4297)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:773)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:249)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:399)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:194)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:148)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:349)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4295)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3421)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:723)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:1424)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:505)\n\tat sun.reflect.GeneratedMethodAccessor527.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file abfss:REDACTED_LOCAL_PART@dlformulaone2024.dfs.core.windows.net/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet. File abfss://demo@dlformulaone2024.dfs.core.windows.net/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:654)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:602)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:747)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:456)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:451)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1983)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:186)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:174)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1697)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dlformulaone2024.dfs.core.windows.net/demo/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet?timeout=90, PathNotFound, \"The specified path does not exist. RequestId:1a85ef3b-e01f-0093-2858-68792a000000 Time:2024-02-26T02:05:28.3676575Z\"\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:617)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:566)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:360)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:298)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.$anonfun$read$2(FileSystemWithMetrics.scala:214)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeMetric(FileSystemWithMetrics.scala:158)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesMetric(FileSystemWithMetrics.scala:178)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.$anonfun$withTimeAndBytesReadMetric$1(FileSystemWithMetrics.scala:192)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.spark.metrics.SamplerWithPeriod.sample(FileSystemWithMetrics.scala:85)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:192)\n\tat com.databricks.spark.metrics.ExtendedTaskIOMetrics.withTimeAndBytesReadMetric$(FileSystemWithMetrics.scala:191)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.withTimeAndBytesReadMetric(FileSystemWithMetrics.scala:199)\n\tat com.databricks.spark.metrics.FSInputStreamWithMetrics.read(FileSystemWithMetrics.scala:213)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.parquet.bytes.BytesUtils.readIntLittleEndian(BytesUtils.java:83)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterBytesFromInputStream(CachingParquetFooterReader.java:135)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:374)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$LegacyCache.read(CachingParquetFooterReader.java:338)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:286)\n\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:282)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:204)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:396)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:502)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:356)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:573)\n\t... 37 more\n",
       "errorSummary": "FileReadException: Error while reading file abfss:REDACTED_LOCAL_PART@dlformulaone2024.dfs.core.windows.net/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet. File abfss://demo@dlformulaone2024.dfs.core.windows.net/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions\nCaused by: FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dlformulaone2024.dfs.core.windows.net/demo/result_managed/part-00000-09105a37-ae49-4bc4-869d-7b154045688b-c000.snappy.parquet?timeout=90, PathNotFound, \"The specified path does not exist. RequestId:1a85ef3b-e01f-0093-2858-68792a000000 Time:2024-02-26T02:05:28.3676575Z\"",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from f1_delta.result_managed\n",
    "version as of 2"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2783025488472319,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4.history_time_travel_vaccum",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

Topics Covered So far,
1.Azure databricks cluster 
2.cluster configuration
3.cluster pool 
4.data bricks utilities
5.accessing ADLS Gen2 from data bricks using Access Keys SAS tokens, Service Principals, Cluster Scoped Authentication
6.Secured Access from Azure Key Vault 
7.Mounting DataLake containers to Databricks
8.Data Ingestion, Processing, Transformation of various file fromats(csv,json, nested and multiline json) from ADLS in databricks with pySpark
9.Databricks Notebook workflows
10.Databricks Jobs 


Access ADLSG2 from Databricks:

1. Access Keys from Containers
2. SAS tokens (Shared Access Tokens) from containers
3. Cluster Scope Access (Not recommended, Setup while configuering the cluster or edit and add spark config of access 
keys or SAS tokens)
4.Azure Active Directory - ADD: Databricks authenticates to ADD thn ADD authenticates to ALDSG2. Configure while creating clusters in advanced options
by ticking Enable credential passthrough for user-level data access.
5. Service Principles


#cluster scope access using access keys for the course.
for cluster scope add folowing config 
fs.azure.account.<account_storage_name>.dfs.core.windows.net {{secrets/
<secreat_scope_name>/<secreat_key>}}

Section: 7 Securing Access to Azure Data Lake

Services for keeping the keys secrets
1. Databricks secret scope managed by databricks. can be confiured by db cli or api 
2. Azure key vault backed secret scope

can be combined to create more rigid solutions for the secret keys and tokens. 
 
##First create azure key valult and add all the secrets, link it with databricks secret scope. After its read 
for notebook, clusters and jobs. to get the secrets use dbutils.secrets.get 

Steps:
create Key valut, add key name and keys in azure. go to databricks onboarding page, edit the url and remove the word 
onboarding, and add '#secrets/createScope'. hit enter. add DNS address and resource id.

DNS: go to key valult then properties valut URI is the dns
Resource ID : is resource ID

# config spark while creating or starting a cluster to access storage acconts.

in advence option of the cluster add a spark config.

'fs.azure.account.key.<storage_acc_name>.dfs.core.windows.net {{secrets/
<scope_name>/<secret_key>}}'


##Section 8: upload files to DBFS FileStore

go to account settings, advanced thn others enable dbfs. go to catelod from sidebar thn brows
DBFS. thn upload files. works like a local store like colab. upload and driect access




# Hive meta store: to treat the data from various files, these files of data are needed to be registered  in a meta store. Databricks use Apache Hive Metastore for this.

For the Hive meta store there are two storage options one is the data bricks default and other is external  meta store like Azure SQL or My SQL etc
Once those tables are registered then they can be access form Spark SQL.



A data bricks workspace can have a number database aka schemas. Databases can have number of tables and views. Two types of tables in Spark. Managed(metadata and files are managed by spark) and External(metadata managed by spark but files are managed externally). 
